{
  "env_id": "Hopper-v2",
  "seed": 1,
  "device": "cuda",
  "save_dir": "data",
  "eval_interval": 2000,
  "log_interval": 10000,
  "weight_decay": 0.005,
  "max_grad_norm": null,
  "batch_size": 64,
  "save_best_only": true,
  "test": false,
  "resume": false,
  "resume_step": null,
  "policy_lr": 0.0003,
  "value_lr": 0.001,
  "num_envs": 2,
  "opt_epochs": 5,
  "normalize_adv": true,
  "clip_vf_loss": true,
  "vf_loss_type": "mse",
  "vf_coef": 0.5,
  "ent_coef": 0.01,
  "clip_range": 0.2,
  "gae_lambda": 0.95,
  "rew_discount": 0.99,
  "max_steps": 2000000.0,
  "episode_steps": 1000,
  "use_amsgrad": false,
  "git_info": {
    "directory": "/home/tao/PycharmProjects/easyrl",
    "code_diff": "diff --git a/easyrl/agents/ppo.py b/easyrl/agents/ppo.py\nindex 5c680a0..0d9adb2 100644\n--- a/easyrl/agents/ppo.py\n+++ b/easyrl/agents/ppo.py\n@@ -44,8 +44,7 @@ class PPOAgent(BaseAgent):\n \n     @torch.no_grad()\n     def get_action(self, ob, sample=True, **kwargs):\n-        self.actor.eval()\n-        self.critic.eval()\n+        self.eval_mode()\n         t_ob = torch.from_numpy(ob).float().to(ppo_cfg.device)\n         act_dist, val = self.get_act_val(t_ob)\n         action = action_from_dist(act_dist,\n@@ -67,13 +66,13 @@ class PPOAgent(BaseAgent):\n             val = self.critic(body_x=body_out)\n         else:\n             val = self.critic(x=ob)\n+        val = val.squeeze(-1)\n         return act_dist, val\n \n     def optimize(self, data, **kwargs):\n-        self.actor.train()\n-        self.critic.train()\n+        self.train_mode()\n         for key, val in data.items():\n-            data[key] = torch.from_numpy(val).float().to(ppo_cfg.device)\n+            data[key] = val.float().to(ppo_cfg.device)\n         ob = data['ob']\n         action = data['action']\n         ret = data['ret']\n@@ -125,17 +124,25 @@ class PPOAgent(BaseAgent):\n                                                        ppo_cfg.max_grad_norm)\n         self.optimizer.step()\n         optim_info = dict(\n-            pg_loss=pg_loss.item,\n-            vf_loss=vf_loss.item,\n-            total_loss=loss.item,\n-            entropy=entropy.item,\n-            approx_kl=approx_kl.item,\n+            pg_loss=pg_loss.item(),\n+            vf_loss=vf_loss.item(),\n+            total_loss=loss.item(),\n+            entropy=entropy.item(),\n+            approx_kl=approx_kl.item(),\n             clip_frac=clip_frac\n         )\n         if grad_norm is not None:\n             optim_info['grad_norm'] = grad_norm\n         return optim_info\n \n+    def train_mode(self):\n+        self.actor.train()\n+        self.critic.train()\n+\n+    def eval_mode(self):\n+        self.actor.eval()\n+        self.critic.eval()\n+\n     def save_model(self, is_best=False, step=None):\n         if not ppo_cfg.save_best_only and step is not None:\n             ckpt_file = Path(ppo_cfg.model_dir) \\\ndiff --git a/easyrl/configs/basic_config.py b/easyrl/configs/basic_config.py\nindex 643ec56..b692d14 100644\n--- a/easyrl/configs/basic_config.py\n+++ b/easyrl/configs/basic_config.py\n@@ -1,12 +1,12 @@\n import json\n import shutil\n-from pathlib import Path\n-\n from dataclasses import asdict\n from dataclasses import dataclass\n+from pathlib import Path\n \n-from easyrl.utils.rl_logger import logger\n from easyrl.utils.common import get_git_infos\n+from easyrl.utils.rl_logger import logger\n+\n \n @dataclass\n class BasicConfig:\n@@ -14,7 +14,7 @@ class BasicConfig:\n     seed: int = 1\n     device: str = 'cuda'\n     save_dir: str = 'data'\n-    eval_interval: int = 50000\n+    eval_interval: int = 2000\n     log_interval: int = 10000\n     weight_decay: float = 0.005\n     max_grad_norm: float = None\ndiff --git a/easyrl/configs/command_line.py b/easyrl/configs/command_line.py\nindex 65b7b65..2784ee8 100644\n--- a/easyrl/configs/command_line.py\n+++ b/easyrl/configs/command_line.py\n@@ -14,5 +14,5 @@ def cfg_from_cmd(cfg):\n \n     args = parser.parse_args()\n     args_dict = vars(args)\n-    for key, val in args_dict:\n+    for key, val in args_dict.items():\n         setattr(cfg, key, val)\ndiff --git a/easyrl/configs/ppo_config.py b/easyrl/configs/ppo_config.py\nindex 9a9e16e..4d1af98 100644\n--- a/easyrl/configs/ppo_config.py\n+++ b/easyrl/configs/ppo_config.py\n@@ -19,7 +19,7 @@ class PPOConfig(BasicConfig):\n     clip_range: float = 0.2\n     gae_lambda: float = 0.95\n     rew_discount: float = 0.99\n-    max_steps: int = 5e6\n+    max_steps: int = 2e6\n     episode_steps: int = 1000\n     use_amsgrad: bool = False\n \ndiff --git a/easyrl/engine/ppo_engine.py b/easyrl/engine/ppo_engine.py\nindex 2155336..9d52500 100644\n--- a/easyrl/engine/ppo_engine.py\n+++ b/easyrl/engine/ppo_engine.py\n@@ -1,5 +1,6 @@\n import time\n \n+import numpy as np\n import torch\n \n from easyrl.configs.ppo_config import ppo_cfg\n@@ -8,7 +9,8 @@ from easyrl.utils.common import list_stats\n from easyrl.utils.gae import cal_gae\n from easyrl.utils.rl_logger import TensorboardLogger\n from easyrl.utils.torch_util import torch_to_np\n-\n+from easyrl.utils.torch_util import EpisodeDataset\n+from torch.utils.data import DataLoader\n \n class PPOEngine(BasicEngine):\n     def __init__(self, agent, env, runner):\n@@ -67,12 +69,14 @@ class PPOEngine(BasicEngine):\n \n     def train_once(self):\n         t0 = time.perf_counter()\n+        self.agent.eval_mode()\n         traj = self.runner(ppo_cfg.episode_steps)\n         rewards = traj.rewards\n         actions_info = traj.actions_info\n         vals = np.array([ainfo['val'] for ainfo in actions_info])\n         log_prob = np.array([ainfo['log_prob'] for ainfo in actions_info])\n-        act_dist, last_val = self.agent.get_act_val(traj[-1].next_ob)\n+        with torch.no_grad():\n+            act_dist, last_val = self.agent.get_act_val(traj[-1].next_ob)\n         adv = cal_gae(gamma=ppo_cfg.rew_discount,\n                       lam=ppo_cfg.gae_lambda,\n                       rewards=rewards,\n@@ -86,15 +90,26 @@ class PPOEngine(BasicEngine):\n             ret=ret,\n             adv=adv,\n             log_prob=log_prob,\n-            val=val\n+            val=vals\n         )\n-        optim_info = self.agent.optimize(data)\n+        rollout_dataset = EpisodeDataset(**data)\n+        rollout_dataloader = DataLoader(rollout_dataset,\n+                                        batch_size=ppo_cfg.batch_size,\n+                                        shuffle=True)\n+        optim_infos = []\n+        for oe in range(ppo_cfg.opt_epochs):\n+            for batch_ndx, batch_data in enumerate(rollout_dataloader):\n+                optim_info = self.agent.optimize(batch_data)\n+                optim_infos.append(optim_info)\n \n+        log_info = dict()\n+        for key in optim_infos[0].keys():\n+            log_info[key] = np.mean([inf[key] for inf in optim_infos])\n         t1 = time.perf_counter()\n-        optim_info['time_per_iter'] = t1 - t0\n-        optim_info['explor_steps_per_iter'] = traj.total_steps\n+        log_info['time_per_iter'] = t1 - t0\n+        log_info['explor_steps_per_iter'] = traj.total_steps\n         self.cur_step += traj.total_steps\n-        log_info = dict()\n-        for key, val in optim_info:\n-            log_info['train/' + key] = val\n-        return log_info\n+        train_log_info = dict()\n+        for key, val in log_info.items():\n+            train_log_info['train/' + key] = val\n+        return train_log_info\ndiff --git a/easyrl/utils/common.py b/easyrl/utils/common.py\nindex ee5e0e6..3ba5ef6 100644\n--- a/easyrl/utils/common.py\n+++ b/easyrl/utils/common.py\n@@ -1,7 +1,8 @@\n-import numpy as np\n import git\n+import numpy as np\n+\n from easyrl.utils.rl_logger import logger\n-from collections import namedtuple\n+\n \n def tile_images(img_nhwc):\n     \"\"\"\ndiff --git a/easyrl/utils/data.py b/easyrl/utils/data.py\nindex 0d18c42..83d913d 100644\n--- a/easyrl/utils/data.py\n+++ b/easyrl/utils/data.py\n@@ -1,9 +1,10 @@\n+from dataclasses import dataclass\n+from dataclasses import field\n from typing import Any\n from typing import Dict\n from typing import List\n \n-from dataclasses import dataclass\n-from dataclasses import field\n+import numpy as np\n \n \n @dataclass\ndiff --git a/easyrl/utils/gae.py b/easyrl/utils/gae.py\nindex 1400e35..da5c169 100644\n--- a/easyrl/utils/gae.py\n+++ b/easyrl/utils/gae.py\n@@ -5,7 +5,7 @@ def cal_gae(gamma, lam, rewards, value_estimates, last_value, dones):\n     advs = np.zeros_like(rewards)\n     last_gae_lam = 0\n     value_estimates = np.concatenate((value_estimates,\n-                                      last_value),\n+                                      last_value.reshape(1, -1)),\n                                      axis=0)\n     for t in reversed(range(rewards.shape[0])):\n         non_terminal = 1.0 - dones[t]\ndiff --git a/easyrl/utils/gym_util.py b/easyrl/utils/gym_util.py\nindex b45834e..2d92960 100644\n--- a/easyrl/utils/gym_util.py\n+++ b/easyrl/utils/gym_util.py\n@@ -3,9 +3,11 @@ import gym\n from easyrl.envs.dummy_vec_env import DummyVecEnv\n from easyrl.envs.shmem_vec_env import ShmemVecEnv\n from easyrl.envs.timeout import TimeOutEnv\n+from easyrl.utils.rl_logger import logger\n \n \n def make_vec_env(env_id, num_envs, seed=1, no_timeout=True, env_kwargs=None):\n+    logger.info(f'Creating {num_envs} environments.')\n     if env_kwargs is None:\n         env_kwargs = {}\n \ndiff --git a/easyrl/utils/rl_logger.py b/easyrl/utils/rl_logger.py\nindex a4b3e15..9677383 100644\n--- a/easyrl/utils/rl_logger.py\n+++ b/easyrl/utils/rl_logger.py\n@@ -124,7 +124,7 @@ class TensorboardLogger:\n \n         for tp in self.supported_types[:3]:\n             if tp in kvs:\n-                for k, v in kvs[tp]:\n+                for k, v in kvs[tp].items():\n                     getattr(self, f'save_{tp}')(k, v, step)\n         for tp in self.supported_types[3:]:\n             if tp in kvs:\ndiff --git a/easyrl/utils/torch_util.py b/easyrl/utils/torch_util.py\nindex fd11bb3..095f828 100644\n--- a/easyrl/utils/torch_util.py\n+++ b/easyrl/utils/torch_util.py\n@@ -6,6 +6,7 @@ from torch.distributions import Independent\n from torch.distributions import Transform\n from torch.distributions import constraints\n from torch.nn.functional import softplus\n+from torch.utils.data import Dataset\n \n \n def soft_update(target, source, tau):\n@@ -82,3 +83,34 @@ class TanhTransform(Transform):\n         # see details in the following link\n         # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n         return 2. * (math.log(2.) - x - softplus(-2. * x))\n+\n+\n+class EpisodeDataset(Dataset):\n+    def __init__(self, **kwargs):\n+        self.data = dict()\n+        for key, val in kwargs.items():\n+            self.data[key] = self._swap_leading_axes(val)\n+        self.length = next(iter(self.data.values())).shape[0]\n+\n+    def __len__(self):\n+        return self.length\n+\n+    def __getitem__(self, idx):\n+        sample = dict()\n+        for key, val in self.data.items():\n+            sample[key] = val[idx]\n+        return sample\n+\n+    def _swap_leading_axes(self, array):\n+        \"\"\"\n+        Swap and then flatten the array along axes 0 and 1\n+\n+        Args:\n+            array (np.ndarray): array data\n+\n+        Returns:\n+            np.ndarray: reshaped array\n+        \"\"\"\n+        s = array.shape\n+        return array.swapaxes(0, 1).reshape(s[0] * s[1],\n+                                            *s[2:])\ndiff --git a/easyrl/version.py b/easyrl/version.py\nindex 99c4176..b8023d8 100644\n--- a/easyrl/version.py\n+++ b/easyrl/version.py\n@@ -1 +1 @@\n-__version__ = '0.0.1'\n\\ No newline at end of file\n+__version__ = '0.0.1'\ndiff --git a/examples/ppo.py b/examples/ppo.py\nindex 542dd01..1863ae7 100644\n--- a/examples/ppo.py\n+++ b/examples/ppo.py\n@@ -1,3 +1,5 @@\n+import torch.nn as nn\n+\n from easyrl.agents.ppo import PPOAgent\n from easyrl.configs.command_line import cfg_from_cmd\n from easyrl.configs.ppo_config import ppo_cfg\n@@ -13,7 +15,7 @@ def main():\n     cfg_from_cmd(ppo_cfg)\n     if ppo_cfg.resume:\n         ppo_cfg.restore_cfg()\n-    env = make_vec_env(ppo_cfg.env, 2)\n+    env = make_vec_env(ppo_cfg.env_id, ppo_cfg.num_envs)\n     env.reset()\n     ob_size = env.observation_space.shape[0]\n     act_size = env.action_space.shape[0]\ndiff --git a/setup.py b/setup.py\nindex bd66fed..2815737 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -13,7 +13,9 @@ def read_requirements_file(filename):\n     with req_file.open('r') as f:\n         return [line.strip() for line in f]\n \n+\n from IPython import embed\n+\n embed()\n packages = find_packages()\n # Ensure that we don't pollute the global namespace.",
    "code_diff_staged": "",
    "commit_hash": "9e034b9832d6883612cfb10f9a15c2a03109c584",
    "branch_name": "master"
  }
}